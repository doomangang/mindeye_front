# MindEye

**MindEye** is a navigation and assistance app designed for **visually impaired individuals**. It utilizes real-time object detection and tactile paving recognition to offer auditory navigation and obstacle warnings, making daily mobility more accessible and independent.

---

## Features

1. **Voice-Assisted Navigation**
   - Provides step-by-step navigation instructions using **Tmap API** for route data.
   - Users can set departure and arrival locations using **voice recognition**.

2. **Accessible UI**
   - Simple, segmented screens for focused functionality.
   - Large buttons, high-contrast modes, and voice feedback for ease of use.

4. **Speech-to-Text (STT) & Text-to-Speech (TTS)**
   - Voice input for commands and settings.
   - Real-time audio feedback enhances accessibility.

---

## Key Technologies

- **Frontend**: React Native (TypeScript)
- **API Integration**: Tmap API for route guidance
- **Design Tools**: Figma for prototyping and wireframes
- **Voice Interaction**: STT and TTS for accessibility

---

## UI Overview

1. **Home Screen**:
   - Central hub for navigation and obstacle detection.
   - Simple layout with audio feedback for intuitive use.

2. **Navigation Interface**:
   - Users set their locations via voice commands.
   - Clear, step-by-step guidance with audio instructions.

---


This is a new [**React Native**](https://reactnative.dev) project, bootstrapped using [`@react-native-community/cli`](https://github.com/react-native-community/cli).
